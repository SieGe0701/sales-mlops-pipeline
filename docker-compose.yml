version: '3.8'

services:
  mlflow-server:
    image: ghcr.io/mlflow/mlflow:v2.11.3
    container_name: mlflow-server
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
      - MLFLOW_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - ./mlflow:/mlflow   # ðŸ‘ˆ Store artifacts and mlruns inside /mlflow
    command: >
      mlflow server 
      --backend-store-uri /mlflow/mlruns 
      --default-artifact-root /mlflow/artifacts 
      --host 0.0.0.0

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags      # Mount DAGs
      - ./mlflow:/mlflow                      # So that Airflow can run train.py
      - ./airflow/logs:/opt/airflow/logs       # Store logs
      - ./airflow/airflow.db:/opt/airflow/airflow.db # Persist sqlite db
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./mlflow:/mlflow
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/airflow.db:/opt/airflow/airflow.db
    command: scheduler
